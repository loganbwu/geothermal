[
["index.html", "Wairakei Stochastic Simulation Research Compendium 1 Home", " Wairakei Stochastic Simulation Research Compendium Logan Wu 2018-09-22 1 Home This compendium contains all the material used to generate the final report. Components: Research Journal: Meeting notes and emails. Python Extraction Code: Exploratory analysis and process data to a config and data spreadsheet. R and RJAGS: Take the data from the config/data sheets and format into JAGS input. Then post-process the output. This document is automatically generated thanks to Bookdown and Rstudio. (I swear, it only took a few hours to convert my source code into this). Compilation requires data in (root)/wairakei_data/*.xlsx, i.e., you won’t be able to run any of the code but it’s been pre-run for you. Raw data is confidential - contact Contact for access. The config file /wairakei_data/config.xlsx also may contain confidential data and is not committed, as is the entire /wairakei_data/ folder. "],
["research-journal.html", "2 Research Journal 2.1 Week Starting March 8th 2.2 Week Starting March 16th 2.3 Week Starting March 23rd 2.4 Week Starting April 12th 2.5 Week Starting April 26th 2.6 Week Starting July 19th 2.7 Week Starting August 9th 2.8 Week Starting August 16th 2.9 Week Starting September 6th 2.10 Week Starting September 13th", " 2 Research Journal Here are the notes I took from our meetings. They kind of peter at by the end, but the length of the note is inversely proportional to the amount of work I did that week. This journal contains some email transcripts between David Dempsey and Contact. 2.1 Week Starting March 8th Notes Familiarise with spreadsheet Find examples of forecasting, optimisation and stochastic simulation in oil &amp; gas industry Choose part of project Documents used: Fifty years of geothermal power generation at Wairakei AEE - 104706 Take Variation Appn 2017_final.pdf contact_email_instructions.pdf Liquid wells (version 1).xlsx Wairakei Tauhara Draft SMP May 2016.pdf WK TH 2016 Annual Report FINAL.pdf Notes: Excel spreadsheet is missing links to SteamTab.xla Some things will take a long time to understand without knowing the original formula. See ‘WK26A’!K12 – maybe if I wrote it out replacing all the cell references with column names I could work it out, but there’s also all the constants. Not sure if it’s a good use of time to understand the physics. Also, there are slight inconsistencies I don’t understand. For example, see ‘WK26A’!O12 and ‘WK26A Trend’!I10. I expected these to be the same data point. Propose doing proof of concept in something else to better facilitate visualisation, optimisation and simulation. Suggest R or Python (no personal preference). Could use SQL but probably overcomplicating. Should be easy to move to Excel afterwards. I do not mind which part I do. Here are the pros and cons of me doing each part: Optimisation Simulation Pros - ENGSCI 355 linear programming - Have programmed heuristics (ENGSCI 255 &amp; 760) - STATS 731 Bayesian Inference - Have programmed simulations in C++/Python Cons - Don’t understand the spreadsheet - Haven’t yet found examples - Don’t yet understand the energy extraction process 2.2 Week Starting March 16th Meeting Model wk255: inputs and outputs. Create Python script to extract data and replicate plots. Then generalize to other sheets o Plots: mf = f(whp) and mf = f(time | whp) Mass flow = f(WHP): Inputs: WHP, outputs: linearly declining res P to predict mass flow in future Look at using regression uncertainty to create a distribution of future mass flows Make reasonable assumptions Missing data points are not important Maybe use Jupyter notebook? Notes Done linear regression on data 2.3 Week Starting March 23rd Meeting Need to fit elliptic curve Make sure it generalises to all wells Input function for making predictions. \\(\\dot{m} = f(\\text{well}, \\text{whp}, \\text{date})\\) Road trip!! To see all there is to see in Wairakei. Second week of break, sometime April 12th-15th? Meeting with Julian March 28th Operations Decline rate done by operator (Christine). Can indicate if something happened in the well (e.g. scaling) to cause observed long-term changes in the decline rate A workover (de-scaling) increases mass flow and pressure Separation pressure determines the fraction of steam and fluid in the mass flow, where \\(\\dot{m} = f(P_\\text{sep})\\). Christine may explain why all the inconsistent equations. Begin by looking at one or two flash vessels and calculating enthalpy. (Out-of-scope) considerations could also include variability in power prices (affects objective) and air temperature (affects efficiency). One goal (objective function) is to get less brine as a proportion of mass flow. Data Well numbers indicate age (lower numbers are older) – wells from similar periods will act similarly ‘PI’ sheet: gives whp. ‘Equation’ sheet: gives decline rates Try to extract data into Python for a more readable form? [Note: crashed my laptop trying to do this and haven’t tried again yet – Logan] Mathematical modelling Try to predict what decisions would be made Predict causes of pressure decline (e.g. decision tree / tree classifier) e.g. local scaling, global pressure drawdown Make a new spreadsheet (General Projection [note: can’t remember what this was]) Enthalpy: Maybe just do a user-controlled constant (as indicated by no trend in the enthalpy graphs). Also make mass flow / enthalpy decline rate manual from regression. It can change periodically. Create a well operating schedule and predict steam flow. WHP → mass flow → steam flow → enthalpy → power. Predict for one year and optimise steam flow s.t. total mass constraints. Discretise over large time periods (e.g. 3 months to begin) Notes Modify regression and plots to operate on multiple wells simultaneously Remove formulae from Generation Projection spreadsheet to make it run faster 2.4 Week Starting April 12th Meeting with Contact in Taupo Current State GE (General Electric) is currently working on a similar project which includes trading – out of scope for us. CSV-formatted data has been made available to them and has been requested for us. Well test data comes from two sources: bore tests and tracer flow tests (TFTs). Bore tests are denoted by three simultaneous data points and are expensive. TFTs are identified by a single test on a day, and are run under normal operating conditions. Liquid wells regression is satisfactory in its current state. Modifying to fit w269 is unnecessary right now. Modelling Validation (regression): check that mass flow predictions match TFTs. N/A, we use the TFTs in the regression training set. Validation (model): check that flow meters downstream from FPs matches FP input flows. Variables (well head pressure): whp is not a decision variable. It is usually taken from the most recent TFT, representing full load. Variables (enthalpy): h is often independent of whp in watery wells, but confirm for all wells in use before going forward with a constant. h also has variability, so confirm with the FP in case there are any short-term fluctuations. Constraints (consent): bioreactor (did I hear this correctly?) limits, mass limits, temperature (heat energy) limits, temporal limits Constraints (physical): FP min/max h, network connectivity, LP/IP FP capacity Scope: For actual optimisation, the entire system (e.g. including Te Mihi) has to be considered because it is all interconnected. Decisions Workover: whether to do a workover (out of scope or later on) Flipping: which plant to send a well to. High h wells are usually sent to Te Mihi, ranked in order of h. Flips only happen when turning a well on (not one that is already running). Well on/off: in general, almost all wells are used. Objectives Maximise power Minimise wasted mass from disk blowout events (difficult to quantify because there are random system trips) 2.5 Week Starting April 26th Meeting Make one-day plan Lit review: Don’t expect 10 pages/30refs WHP: More realistic one might take varying whps. Notes What about a greedy heuristic / complete enumeration? Would make it easier to do the stochastic simulation if every component is represented by an object Resource consent? Get all the data Split tasks What constraints exist? Chemical/mass/physical(fractions) Data for each well on chemicals Parasitic power needs Confints/prior dists Questions The project is focusing in on flash plants 14, 15, and 16: What constraints are there on the maximum mass flows entering these plants? (for instance, they presumably couldn’t handle a million tons per hour…) What constraints are there on steam flow exiting the plant? Condensed water? The current steamflow limit of the vessels: FP14 - 525 t/h FP15 - 775 t/h FP16 IP - 420 t/h FP16 IP+ - 450 t/h The tools used to monitor mass flow in and out of the flash plants: What uncertainty is attached to these measurements? \\[\\pm\\] 1%? 10%? 0.01%? in terms of value, 10% is acceptable; in terms of trend, it has to be 100% accurate What change in mass flow is required to trip the automatic data logger, so that it begins recording a new value? (I forgot the name of this system…) we don’t have this and I have never heard of this. We do the accuracy check manually using spreadsheets Regarding the power plants (Wairakei, Te Mihi and Poihipi): We have some numbers for the conversion factor from steam to MW – what uncertainty is attached to these? \\[\\pm\\] 5%? Yes the changes to this number is up to 5%, although this number if often calibrated with actual values. Does it depend on the time of year (ambient temperature)? No, we do a different correction for ambient temperature and seasonal changes What constraints are there operating the power plants? I remember discussing that turbines were MW rated and you couldn’t go much above the rating – how much above? The max MWE is the equivalent of the steamflow limits Regarding the TFT and well test measurements: What level of confidence/uncertainty would you attach that these numbers are correct? \\(\\pm\\) 5%-10% is acceptable as long as the trend is similar Do you know if these are publically available? We have been unable to find them. Maybe not. I will confirm with Warren 2.6 Week Starting July 19th Well MFs are calculated. Only FP mass flows are measured 2.7 Week Starting August 9th Verify Bayesian with analytic calculations Questions Can I please confirm there are only steam flow limits on flashplants 14, 15 and 16? not any water flow limits? The limits of the flash plants are specified in terms of steamflow values and not in water flow. This means that if the wells become watery overtime, there will be a lot of excess water flow to be dealt with when fully loading the vessels. With FP16 we have been given limits for FP16 IP and IP+, I can see from the steamfield schematic diagram that it seems as though some wells feed to FP16 IP and some to FP16 IP+ and then some also goes from FP16 IP+ to FP16 IP. Would it be possible to get a bit more an explanation as to how this works, I can’t seem to discern anything from the ‘Data for AU’ spreadsheet provided, this spreadsheet seems to just treat it as one FP. Te Mihi can be a 3-flash system or a 2-flash system. If configured as a 3-flash system: FP16ip+ having the highest separation pressure FP16 IP &amp; FP15 IP second highest LP vessels. The water from IP+ will join the mass going to FP16 &amp; FP15, and then the water from the latter will go to the LP vessel. If configured as a 2-flash system, IP+ water will join the water coming out from FP16 &amp; FP15 . The combined water will go to the LP vessel. How do we determine the amount of steam going to Poihipi, from the ‘Data for AU’ spreadsheet it seems that wells 253, 258, 259, 260 go to Poihipi, but without clarification on the question above I am unsure about this. Steam going to Poihipi will be the steam from IP+, which is NOT ONLY WK253 WK258 WK259 WK260 but also the swinging wells WK270 WK271 WK272 WK245 WK263 WK264 WK265 if they are swung to Te Mihi/ip+. On the schematic diagram there is a well numbered 249 that isnt in the ‘Data for AU’ spreadsheet and conversely there is a well 238 on the ‘Data for AU’ spreadsheet that isnt on the schematic. WK249 has never been used due to high gas. WK238 is actually in the schematic diagram (above WK247 in figure below), although I am not sure if we are referring to the same diagram. If there is a way to determine how much water goes from FP14 to the binary power plant, for example is it a set proportion of the water, or a set number of tonnes per hour? That water should be the total water flow coming out from the LP vessel, which will be dictated by the LP vessel limit. As already provided to you, FP14 LP vessel is limited to 120t/h from 15Mar2018. Prior to 15Mar18, the limit was 40t/h. Water in excess of the LP vessel limit is not included in the calculation for the flow to the binary. I would also like to add a new limit to the FP14 total IP SF=525 t/h. 2.8 Week Starting August 16th Meeting Things for report: discussion of recommendation and model discussion of internal and external considerations/factors 2.9 Week Starting September 6th 2.10 Week Starting September 13th Meeting Switch data to before Dec 1st and make predictions. There is a difference between predicting with PI data and predicting with production curves. Compare TS models and predictions Narrow down problem wells Screenshots of user interface "],
["python-extraction-code.html", "3 Python Extraction Code 3.1 Pre-processing 3.2 Preview Data", " 3 Python Extraction Code The following data extraction and cleaning code is from simulation.ipynb. This file used to be where I did exploratory analysis of the data and tried some models. The Python notebook is also used to generate a rudimentary config file, which I later added additional data to (note to self: don’t run it or it will overwrite!). It seems that now Rmarkdown nicely integrates Python, but the Reticulate package was only released in August 2018. Oh well. I’ve put the Python code in here so technically, if I ran this notebook right this moment, it would do it all. 3.1 Pre-processing (Unix) Launch with cd src jupyter notebook Notes read_binary_solution is able to read from Vida’s outputs and turn it into a well/FP mapping. However I don’t use it now because she doesn’t map all of the wells. Instead I manually enter it from Data for AU.xlsx. Here we read in the Liquid Wells spreadsheet. My script manages to pick up at least 30 of the approx. 60 wells in here, but I manually copy/pasted the rest because it was too annoying. The network structure and parameters are read from a config spreadsheet, config.xlsx. I generate as much as I can automatically and then fill any gaps manually. import pandas as pd import numpy as np import seaborn as sns from datetime import datetime, timedelta import matplotlib.pyplot as plt from matplotlib.colors import Normalize from matplotlib.colorbar import ColorbarBase from IPython.display import display, HTML import itertools import os import pyjags import warnings base_year = &#39;2000&#39; # numeric dates calculated from Jan-01 configpath = &#39;../wairakei_data/config.xlsx&#39; def read_binary_solution(path=&#39;../wairakei_data/toy-network-v4.xlsm&#39;): # read from Vida&#39;s toy model workbook xlfile = pd.ExcelFile(path) sheet = xlfile.parse(&#39;Full LP&#39;) sheet = sheet.loc[sheet.count(1)&gt;50] # arbitrary, anything works sheet = sheet.transpose() sheet.columns = [&#39;used&#39;, &#39;combination&#39;] combinations = pd.DataFrame([x.split(&#39;-&#39;) for x in sheet.query(&#39;used==1&#39;)[&#39;combination&#39;]], columns = [&#39;well&#39;, &#39;fp&#39;]) combinations[&#39;well&#39;] = &#39;wk&#39; + combinations[&#39;well&#39;] combinations[&#39;fp&#39;] = &#39;fp&#39; + combinations[&#39;fp&#39;] return(combinations) def myprint(df): display(HTML(df.to_html())) def central(data, m=3.29): return data[abs(data - np.mean(data)) &lt; m * np.std(data)] def rename_wk(names): # fix &#39;WK&#39; inconsistencies new = names.str.lower() new = new.str.replace(&quot;^[^\\d]*&quot;, &quot;wk&quot;) new = new.str.strip() return new def datetime_to_numeric(my_datetime): # returns days since base_year-01-01. try: date_numeric = (my_datetime - datetime(int(base_year),1,1)) / timedelta(days=1) # datetime implem except: date_numeric = (my_datetime - np.datetime64(base_year)) / np.timedelta64(1, &#39;D&#39;) # numpy implem return date_numeric # Check if Excel file is already in memory (loading is slow) try: xl except: xl = pd.ExcelFile(&#39;../wairakei_data/Liquid wells (version 1).xlsx&#39;) sheetlist = [x for x in xl.sheet_names if set(x) &amp; set(&#39;FtT(L&#39;) == set()] print(&quot;Sheets:&quot;, &#39;, &#39;.join(sheetlist)) # sheets to load data from Sheets: WK26A, WK26B, WK27 curve, WK28, WK46, WK55 curve, WK59, WK66 curve, WK67 curve, WK68, WK70, WK71, WK72, WK74, WK76, WK81 curve, WK82, WK83, WK88, WK92, WK96, WK101, WK116, w124, WK207, WK215, WK222, WK229, wk242 , wk243, wk244 , wk245 , wk247, w253, w254, wk255, wk256, w258, w259, w260, w261, w262, wk263, w264, w265, w266, w267, w268, w269, WK270, WK271, WK272, WK216, WK65, WK118, 253 sheets = [&#39;wk255&#39;, &#39;wk256&#39;] dfs = [] for sheet in sheets: try: df = xl.parse(sheet) # select well data df[&#39;well&#39;] = sheet # label data with well name dfs.append(df) except: print(&#39;Failed on sheet&#39;, sheet) df = pd.concat(dfs) df = df[[&#39;date&#39;, &#39;whp&#39;, &#39;mf&#39;, &#39;h&#39;, &#39;well&#39;]] # only keep certain columns df[&#39;well&#39;] = rename_wk(df[&#39;well&#39;]) df[&#39;mf&#39;] = pd.to_numeric(df[&#39;mf&#39;], errors=&#39;coerce&#39;) # remove &#39;dummy&#39; entries df = df.dropna(subset=[&#39;date&#39;, &#39;whp&#39;, &#39;mf&#39;]) # remove NA df[&#39;date_numeric&#39;] = datetime_to_numeric(df[&#39;date&#39;]) # yrs since base_year df = df.reset_index(drop=True) wells = df[&#39;well&#39;].unique() print(df.head()) date whp mf h well date_numeric 0 2008-12-18 13.028442 507.776080 1030.000000 wk255 3274.0 1 2008-12-18 14.841479 283.165920 1030.000000 wk255 3274.0 2 2008-12-18 14.939021 208.716385 1030.000000 wk255 3274.0 3 2009-04-08 13.466667 498.071714 1054.230235 wk255 3385.0 4 2009-04-08 13.800000 457.879864 1040.519288 wk255 3385.0 regression_df = df.reset_index(drop=True) wells = regression_df[&#39;well&#39;].unique() print(wells) # import and process data [‘wk255’ ‘wk256’] try: fpxl except: fpxl = pd.ExcelFile(&#39;../wairakei_data/Data for AU.xlsx&#39;) fpdf = pd.read_excel(fpxl, &#39;calculation&#39;, header=1, usecols=&quot;D:E, J:L, N:P&quot;) fpdf = fpdf.rename(columns={&quot;FP15&quot;: &quot;well&quot;, &quot;Unnamed: 1&quot;: &quot;fp&quot;, &quot;hf&quot;: &quot;hf_ip&quot;, &quot;hg&quot;: &quot;hg_ip&quot;, &quot;hfg&quot;: &quot;hfg_ip&quot;, &quot;hf.1&quot;: &quot;hf_lp&quot;, &quot;hg.1&quot;: &quot;hg_lp&quot;, &quot;hfg.1&quot;: &quot;hfg_lp&quot;}) fpdf = fpdf[pd.to_numeric(fpdf[&#39;hf_ip&#39;], errors=&#39;coerce&#39;).notnull()] # make sure it has the necessary data for col in [&#39;well&#39;, &#39;fp&#39;]: fpdf[col] = fpdf[col].str.lower() fpdf[fpdf.columns] = fpdf[fpdf.columns].apply(pd.to_numeric, errors=&#39;ignore&#39;) print(fpdf.head()) well fp hf_ip hg_ip hfg_ip hf_lp hg_lp \\ 0 wk255 fp15 677.625311 2757.984943 2080.359632 461.792989 2691.196937 1 wk256 fp15 677.625311 2757.984943 2080.359632 461.792989 2691.196937 2 wk251 fp15 677.625311 2757.984943 2080.359632 461.792989 2691.196937 3 wk250 fp15 677.625311 2757.984943 2080.359632 461.792989 2691.196937 4 wk252 fp15 677.625311 2757.984943 2080.359632 461.792989 2691.196937 hfg_lp 0 2229.403949 1 2229.403949 2 2229.403949 3 2229.403949 4 2229.403949 def write_config(configpath): # only use if it gets lost. Will refresh file well_fp_map = pd.DataFrame({&#39;well&#39;: [&#39;wk27&#39;, &#39;wk242&#39;, &#39;wk247&#39;, &#39;wk253&#39;, &#39;wk254&#39;, &#39;wk255&#39;, &#39;wk256&#39;, &#39;wk258&#39;, &#39;wk259&#39;, &#39;wk267&#39;, &#39;wk268&#39;, &#39;wk269&#39;, &#39;wk270&#39;, &#39;wk271&#39;, &#39;wk272&#39;], &#39;fp&#39;: [&#39;fp1&#39;, &#39;fp14&#39;, &#39;fp15&#39;, &#39;fp16&#39;, &#39;fp16&#39;, &#39;fp15&#39;, &#39;fp15&#39;, &#39;fp16&#39;, &#39;fp16&#39;, &#39;fp16&#39;, &#39;fp16&#39;, &#39;fp15&#39;, &#39;fp15&#39;, &#39;fp14&#39;, &#39;fp14&#39;]}, columns=[&#39;well&#39;, &#39;fp&#39;]) fp_gen_map = pd.DataFrame({&#39;fp&#39;: [&#39;abandoned&#39;, &#39;poi dry&#39;, &#39;direct ip&#39;, &#39;fp1&#39;, &#39;fp14&#39;, &#39;fp15&#39;, &#39;fp16&#39;, &#39;fp2&#39;, &#39;fp4&#39;, &#39;fp5&#39;, &#39;fp9-10&#39;], &#39;gen_ip&#39;: [ None, &#39;POI&#39;, None, &#39;WRK&#39;, &#39;WRK&#39;, &#39;THI&#39;, &#39;POI&#39;, &#39;WRK&#39;, &#39;WRK&#39;, &#39;WRK&#39;, &#39;WRK&#39; ], &#39;gen_lp&#39;: [ None, &#39;POI&#39;, None, &#39;WRK&#39;, &#39;WRK&#39;, &#39;THI&#39;, &#39;POI&#39;, &#39;WRK&#39;, &#39;WRK&#39;, &#39;WRK&#39;, &#39;WRK&#39; ], &#39;gen_w&#39;: [ None, None, None, &#39;BIN&#39;, None, None, None, &#39;BIN&#39;, &#39;BIN&#39;, &#39;BIN&#39;, &#39;BIN&#39; ]}, columns=[&#39;fp&#39;, &#39;gen_ip&#39;, &#39;gen_lp&#39;, &#39;gen_w&#39;]) gen_constants = pd.DataFrame({&#39;gen&#39;: [&#39;WRK&#39;, &#39;THI&#39;, &#39;BIN&#39;, &#39;POI&#39; ], &#39;ip&#39;: [ True, True, False, True ], &#39;lp&#39;: [ True, True, False, True ], &#39;bin&#39;: [ False, False, True, False], &#39;factor&#39;: [ 9.2, 8.22, 178.9, 7.76]}, # m3/MW columns=[&#39;gen&#39;, &#39;ip&#39;, &#39;lp&#39;, &#39;bin&#39;, &#39;factor&#39;]) # find details of the last known operating conditions last_idx = regression_df.groupby(&#39;well&#39;)[&#39;date_numeric&#39;].idxmax() operating_conditions = regression_df.iloc[last_idx][[&#39;well&#39;, &#39;whp&#39;, &#39;h&#39;]] # set constants (could use median) fp_constants = fpdf.groupby(&#39;fp&#39;).mean().reset_index() if os.path.exists(configpath): os.remove(configpath) config_writer = pd.ExcelWriter(&#39;../wairakei_data/config.xlsx&#39;) print(&quot;Writing config data to&quot;, configpath) configdata = {&#39;well_fp_map&#39;: well_fp_map, &#39;fp_gen_map&#39;: fp_gen_map, &#39;operating_conditions&#39;: operating_conditions, &#39;fp_constants&#39;: fp_constants, &#39;gen_constants&#39;: gen_constants} for sheetname, df in configdata.items(): df.to_excel(config_writer, sheetname, index=False) # config_writer.save() # Don&#39;t overwrite! return pd.ExcelFile(configpath) try: config = pd.ExcelFile(configpath) except FileNotFoundError: print(&quot;Warning: are you sure you want to overwrite the config file?&quot;) # config = write_config(configpath) ## Exploratory Analysis import itertools cmap = plt.get_cmap(&#39;viridis&#39;) fig, (ax1, ax2) = plt.subplots(1,2, figsize=[14,4]) fig.tight_layout() #spreads out the plots # left plot (not that useful tbh) df.plot(&#39;date&#39;, &#39;whp&#39;, style=&#39;x&#39;, ax=ax1) ax1.set_xlabel(&#39;date&#39;) ax1.set_ylabel(&#39;whp&#39;) # right plot (different colours represent time) marker = itertools.cycle([&#39;o&#39;, &#39;,&#39;, &#39;+&#39;, &#39;x&#39;, &#39;*&#39;, &#39;.&#39;]) for well in wells: plt.scatter(&#39;whp&#39;, &#39;mf&#39;, c=&#39;date_numeric&#39;, data=df.loc[df[&#39;well&#39;]==well], marker=next(marker), label=well) ax2.set_xlabel(&#39;whp&#39;) ax2.set_ylabel(&#39;mf&#39;) plt.legend() plt.show() ## Set up regression data and create prediction frame for plotting date_pred = np.arange(df[&#39;date&#39;].min(), df[&#39;date&#39;].max(), np.timedelta64(365*2, &#39;D&#39;).astype(datetime)) date_numeric_pred = datetime_to_numeric(date_pred) whp_pred = np.linspace(0, 16, 1000) well_pred = wells pred = pd.DataFrame(list(itertools.product(date_numeric_pred, whp_pred, well_pred)), columns=[&#39;date_numeric&#39;, &#39;whp&#39;, &#39;well&#39;]) print(pred.head(3)) ## Perform regression and prediction ## date_numeric whp well ## 0 3274.0 0.000000 wk255 ## 1 3274.0 0.000000 wk256 ## 2 3274.0 0.016016 wk255 from statsmodels.formula.api import ols # Not conditioned on date model1 = ols(&quot;mf ~ well * whp&quot;, data=df) results1 = model1.fit() pred[&#39;mf1&#39;] = results1.predict(pred) # Linear fit dependent on date model2 = ols(&quot;mf ~ well * (whp + date_numeric)&quot;, data=df) results2 = model2.fit() pred[&#39;mf2&#39;] = results2.predict(pred) # Elliptic fit dependent on date model3 = ols(&quot;np.power(mf,2) ~ well * (np.power(whp,2) + date_numeric)&quot;, data=df) results3 = model3.fit() pred[&#39;mf3^2&#39;] = results3.predict(pred) pred.loc[pred[&#39;mf3^2&#39;] &lt; 0, &#39;mf3^2&#39;] = np.nan # remove invalid results pred[&#39;mf3&#39;] = np.sqrt(pred[&#39;mf3^2&#39;]) print(pred.head(3)) # =============================================================== # Set up axes # =============================================================== # colors ## date_numeric whp well mf1 mf2 mf3^2 \\ ## 0 3274.0 0.000000 wk255 798.291912 2116.142850 960405.251630 ## 1 3274.0 0.000000 wk256 524.858769 1878.420017 910600.744186 ## 2 3274.0 0.016016 wk255 797.774653 2114.209523 960404.240037 ## ## mf3 ## 0 980.002679 ## 1 954.254025 ## 2 980.002163 cmap = plt.get_cmap(&#39;viridis&#39;) indices = np.linspace(0, cmap.N, len(df)) my_colors = [cmap(int(i)) for i in indices] # subplots fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=[14,4], gridspec_kw={&#39;width_ratios&#39;: [9,9,9,1]}) ax1.get_shared_y_axes().join(ax1, ax2, ax3) ax1.set_ylim([0, 1000]) ax1.set_xlim(0, 16) ax1.set_ylabel(&#39;Mass flow&#39;) ax1.set_xlabel(&quot;Well head pressure&quot;) ax1.set_title(&#39;$mf \\sim whp$&#39;) ax2.set_title(&#39;$mf \\sim whp + date$&#39;) ax3.set_title(&#39;$mf^2 \\sim whp^2 + date$&#39;) # create date colorbar indices = np.linspace(0, cmap.N, len(date_pred)) my_colors = [cmap(int(i)) for i in indices] norm = Normalize(np.min(df[&#39;date&#39;]).year, np.max(df[&#39;date&#39;]).year) cb = ColorbarBase(ax4, cmap=cmap, norm=norm, orientation=&#39;vertical&#39;) cb.set_label(&#39;Year&#39;) linestyles = itertools.cycle((&#39;-&#39;, &#39;--&#39;, &#39;-.&#39;, &#39;:&#39;)) marker = itertools.cycle([&#39;o&#39;, &#39;,&#39;, &#39;+&#39;, &#39;x&#39;, &#39;*&#39;, &#39;.&#39;]) # =============================================================== # Plot data # =============================================================== # plot raw data points for well in wells: mkr = next(marker) for ax in [ax1, ax2, ax3]: ax.scatter(&#39;whp&#39;, &#39;mf&#39;, c=&#39;date_numeric&#39;, data=df.loc[df[&#39;well&#39;]==well], marker=mkr, label=well) # plot fitted curves for well in wells: lty = next(linestyles) # model 1 # &#39;data&#39; argument filters the data to just the data from one well ax1.plot(&#39;whp&#39;, &#39;mf1&#39;, lty, data=pred[(pred[&#39;well&#39;]==well)]) # models 2 &amp; 3 for i, date in enumerate(date_numeric_pred): # &#39;data&#39; argument similar, for a specific prediction date in the loop # ax2.plot(&#39;whp&#39;, &#39;mf2&#39;, lty, data=pred[(pred[&#39;well&#39;]==well) &amp; pred[&#39;date_numeric&#39;]==date)], c=my_colors[i]) # ax3.plot(&#39;whp&#39;, &#39;mf3&#39;, lty, data=pred[(pred[&#39;well&#39;]==well) &amp; pred[&#39;date_numeric&#39;]==date)], c=my_colors[i]) pass # show model selection criteria for ax, result in zip([ax1, ax2, ax3], [results1, results2, results3]): ax.legend([&#39;Adj $R^2$: %.2f&#39; % result.rsquared_adj, &#39;AIC: %.2f&#39; % result.aic], handlelength=0, handletextpad=0, loc=1).legendHandles[0].set_visible(False) plt.show() 3.2 Preview Data Let’s have a look at what we get out of the pre-processing. We begin with the regression data. Now, the flash plant operating conditions. Here’s a (deterministic) example of what we expect to get out - mass flow predictions per well. But we will use JAGS to get stochastic samples rather than this regression. "],
["r-and-rjags.html", "4 R and RJAGS 4.1 Setup 4.2 Data Handling 4.3 Model 4.4 Convergence Tests 4.5 Posteriors 4.6 Advanced Analysis", " 4 R and RJAGS From the report introduction: There are two advantages to our proposed system compared with the current workflow: Our system combines data from multiple sources into a statistical model that includes uncertainty using Bayesian statistics. The operator can interact with the internal model through Excel to conduct scenario analysis and automatically visualise the results. — Logan This Rmarkdown-generated page will serve as proof that a fully automated proof of concept has been developed. Whether the code is sufficiently commented or not … is a different question. 4.1 Setup configpath = &#39;../wairakei_data/config.xlsx&#39; regdatapath = &#39;../wairakei_data/data.xlsx&#39; extraliqregpath = &quot;../wairakei_data/extra_liq.csv&quot; # for regression extradatapath = &quot;../wairakei_data/well_pi.csv&quot; # ts data pipath &lt;- &quot;../wairakei_data/short version Generation Projection 2016.xlsx&quot; base_year = &#39;2000&#39; prediction_date = &#39;2017-12-01&#39; production_curve_wells = c(&#39;wk255&#39;, &#39;wk263&#39;) tsplotwells = c(&quot;wk118&quot;, &quot;wk216&quot;, &quot;wk605&quot;) decline_wells = c(production_curve_wells, &quot;wk272&quot;, &quot;wk86&quot;, &quot;wk116&quot;) base_datetime = as.POSIXct(paste(base_year, 1, 1, sep=&#39;-&#39;)) today_datetime = as.POSIXct(prediction_date) # theme_update(text=element_text(family=&quot;Times New Roman&quot;)) &#39;%ni%&#39; &lt;- Negate(&#39;%in%&#39;) # for over-plotting special_wells = c(production_curve_wells, tsplotwells, &quot;wk86&quot;, &quot;wk116&quot;) use.censor = F n_steps = 1000 censor = function(x, type) { # Hash the facility identifier (beware of hash clashes) if (!use.censor) { return(x) } else if (type==&quot;well&quot;) { return(paste0(&quot;w&quot;, toupper(substr(sha1(x), 1, 3)))) } else if (type==&quot;fp&quot;) { return(paste0(&quot;fp&quot;, toupper(substr(sha1(x), 1, 2)))) } } 4.2 Data Handling Data is extracted and cleaned using Python in simulation.ipynb. The Python notebook is also used to generate a rudimentary config file, but some things (network connectivity) are specified manually. R is used to: Read raw data and config from Excel/CSV files Do additional pre-processing that depends on the data available Censor sensitive facility names Create a graph structure Make the data into a JAGS-readable format 4.2.1 Load Data Reads data from several spreadsheets, including PI data. PI data is special because it has not been pre-processed. It requires additional filtering and basic pre-processing. # read in config data configsheets = excel_sheets(configpath) for (sheet in configsheets) { assign(sheet, read_excel(configpath, sheet)) } stopifnot(!anyDuplicated(well_fp_map$well)) # each well cannot map to multiple flash plants # read in PI data PI &lt;- read_excel(pipath, &quot;From PI sheet&quot;, skip=1) %&gt;% rename(facility = Unit, variable = X__1, id = X__2, description = X__3, code = X__4) %&gt;% gather(key=&quot;datechar&quot;, value=&quot;value&quot;, -c(facility, variable, id, description, code)) %&gt;% mutate(date = as.Date(as.numeric(datechar), origin = &quot;1899-12-30&quot;), value = as.numeric(value)) %&gt;% select(-c(datechar, id)) %&gt;% mutate_if(is.character, tolower) %&gt;% mutate(value = as.numeric(value)) %&gt;% drop_na(value) %&gt;% filter(date &gt;= as.Date(&quot;2017-11-01&quot;), date &lt; as.Date(prediction_date)) %&gt;% filter(!str_detect(variable, &quot;condition|calc&quot;)) %&gt;% filter(str_detect(facility, &quot;wk&quot;)) extra_liq &lt;- PI %&gt;% select(facility, date, variable, value) %&gt;% # filter(value&gt;1e-4) %&gt;% filter(str_detect(variable, &quot;plot|phase|whp|flow&quot;)) %&gt;% spread(key=variable, value=value) %&gt;% mutate(mf = pmax(`2phase flow`, `fp14 plot flow`, `fp15 plot flow`, `flow`, na.rm=T), whp = pmax(`fp14 plot whp`, `fp15 plot whp`, `fp16 plot whp`, `whp`, na.rm=T), source = &quot;PI Database&quot;) %&gt;% select(well=facility, date, whp, mf, source) %&gt;% drop_na() # read in regression data (plus extra) regression_df = read_excel(regdatapath) %&gt;% mutate(source=&quot;Well Tests&quot;) dry_df = PI %&gt;% filter(str_detect(facility, &quot;wk&quot;)) %&gt;% select(facility, date, variable, value) %&gt;% # filter(value&gt;1e-2) %&gt;% group_by(facility, date) %&gt;% spread(key=variable, value=value) %&gt;% select(facility, date, `ip sf`, `actual massflow`) %&gt;% gather(key=&quot;key&quot;, value=&quot;mf&quot;, `ip sf`, `actual massflow`) %&gt;% ungroup() %&gt;% drop_na() %&gt;% rename(well=facility) 4.2.2 Censor names Censor well and flash plant names using a hash algorithm. Change the flag in setup to disable. dry_df$well = censor(dry_df$well, &quot;well&quot;) extra_liq$well = censor(extra_liq$well, &quot;well&quot;) fp_constants$fp = censor(fp_constants$fp, &quot;fp&quot;) fp_gen_map$fp = censor(fp_gen_map$fp, &quot;fp&quot;) operating_conditions$well = censor(operating_conditions$well, &quot;well&quot;) regression_df$well = censor(regression_df$well, &quot;well&quot;) well_fp_map$well = censor(well_fp_map$well, &quot;well&quot;) well_fp_map$fp = censor(well_fp_map$fp, &quot;fp&quot;) production_curve_wells = censor(production_curve_wells, &quot;well&quot;) special_wells = censor(special_wells, &quot;well&quot;) tsplotwells = censor(tsplotwells, &quot;well&quot;) 4.2.3 Preprocessing Generate metadata, such as which wells have which data sources, and translate facility names into unique integer IDs. Also creates dummy facilities for multiple purposes. # combine with extra regression_df = plyr::rbind.fill(regression_df, extra_liq) regression_df = regression_df %&gt;% mutate(date_numeric = as.numeric(date - base_datetime)) %&gt;% mutate(date_numeric=ifelse(date_numeric&gt;0, date_numeric, NA)) # remove dates before baseline dry_df = dry_df %&gt;% filter(well %ni% unique(regression_df$well)) %&gt;% mutate(date_numeric = as.numeric(as.POSIXct(date) - base_datetime)) %&gt;% mutate(date_numeric=ifelse(date_numeric&gt;0, date_numeric, NA)) # remove dates before baseline well_fp_map = well_fp_map %&gt;% select(well, fp) %&gt;% drop_na() # today_numeric = (Sys.time() - base_datetime) %&gt;% as.numeric() today_numeric = (today_datetime - base_datetime) %&gt;% as.numeric() # assign unique facility IDs liq_wells = unique(regression_df$well) # aka production curve wells dry_wells = unique(dry_df$well) # aka time series wells map_wells = unique(well_fp_map$well) # any well mapped in config well_names = unique(c(liq_wells, dry_wells)) fp_names = c(well_fp_map$fp, fp_gen_map$fp, fp_constants$fp) %&gt;% unique() fluid_types = c(&#39;ip&#39;, &#39;lp&#39;, &#39;w&#39;) gen_names = gen_constants$gen %&gt;% unique() %&gt;% sort() ip_gen_names = paste(gen_names, &#39;ip&#39;, sep=&#39;_&#39;) lp_gen_names = paste(gen_names, &#39;lp&#39;, sep=&#39;_&#39;) w_gen_names = paste(gen_names, &#39;w&#39;, sep=&#39;_&#39;) dummy_gen_names = c(ip_gen_names, lp_gen_names, w_gen_names) %&gt;% sort() all_names = c(&#39;DUMMY&#39;, well_names, fp_names, dummy_gen_names, gen_names) ids = 1:length(all_names) names(ids) = all_names # check data quality no_data_wells = map_wells[!map_wells %in% c(liq_wells, dry_wells)] # see which ones we&#39;re completely guessing for no_map_wells = c(liq_wells, dry_wells)[!c(liq_wells, dry_wells) %in% map_wells] missing = data.frame(Wells = c(paste(no_map_wells, collapse=&quot;, &quot;)), row.names = c(&quot;Data available but no FP&quot;)) print(xtable(missing, type = &quot;latex&quot;, caption=paste0(&quot;Potential data quality issues. &quot;, names(ids)[71], &quot; is known to be not connected, and &quot;, names(ids)[31], &quot; has an A/B pairing with &quot;, names(ids)[32], &quot;.&quot;), label=&quot;tab:quality&quot;), file = &quot;../_media/quality.tex&quot;) # add names in data with IDs regression_df = regression_df %&gt;% mutate(well_id=ids[well]) dry_df = dry_df %&gt;% mutate(well_id=ids[well]) operating_conditions = operating_conditions %&gt;% mutate(well_id=ids[well]) %&gt;% rename(whp_pred=whp) fp_constants = fp_constants %&gt;% mutate(fp_id=ids[fp]) gen_constants = gen_constants %&gt;% mutate(gen_id=ids[gen]) %&gt;% select(-gen) well_fp_map = well_fp_map %&gt;% mutate(well_id=ids[well], fp_id=ids[fp]) %&gt;% select(-c(well, fp)) fp_gen_map = fp_gen_map %&gt;% mutate(fp_id=ids[fp], gen_ip_id=ids[gen_ip], gen_lp_id=ids[gen_lp], gen_w_id=ids[gen_w]) %&gt;% select(-c(fp, gen_ip, gen_lp, gen_w)) incomplete.fps = unique(well_fp_map %&gt;% filter(is.na(well_id)) %&gt;% mutate(fp = names(ids)[fp_id]) %&gt;% pull(fp)) 4.2.4 Graph Work out which of the (now uniquely integer-identified) facilities flows to which. Then generates a graphic to check for correctness. # create connectivity matrix. i flows to j # wells to FPs v = matrix(0, nrow=length(ids), ncol=length(ids)) v[1,-1] = 1 for (i in 1:nrow(well_fp_map)) { id_i = well_fp_map[[i, 1]] id_j = well_fp_map[[i, 2]] v[id_i, id_j] = 1 } # send ip/lp/w flows to dummy gens for (i in 1:nrow(fp_gen_map)) { id_i = fp_gen_map[[i, 1]] for (j in 2:ncol(fp_gen_map)) { facility_j = names(ids)[fp_gen_map[[i, j]]] facility_dummy_j = paste(facility_j, fluid_types[j-1], sep=&#39;_&#39;) id_j = ids[facility_dummy_j] if (!is.na(id_j)) { v[id_i, id_j] = 1 } } } # dummy gens to gens for (i in 1:nrow(gen_constants)) { id_j = gen_constants$gen_id[i] facility_j = names(ids)[id_j] for (fluid in fluid_types) { facility_dummy_i = paste(facility_j, fluid, sep=&#39;_&#39;) id_i = ids[facility_dummy_i] v[id_i, id_j] = 1 } } # convert form m = matrix(0, nrow=nrow(v), ncol=max(colSums(v))) rownames(m) = all_names for (i in 1:nrow(v)) { for (j in 1:ncol(v)) { if (v[[i, j]]==1) { m[j, sum(m[j,]&gt;0)+1] = i } } } flows_to = function(well) { return(names(ids)[m[well,]][-1]) } # generate coordinates dummy_locs = data.frame(name=&#39;DUMMY&#39;, x=-0.1, y=0) well_locs = data.frame(name=well_names, x=0, y=seq(1, 1/(length(well_names)-1), length.out=length(well_names))) fp_locs = data.frame(name=fp_names, x=1, y=seq(0, 1, length.out=length(fp_names))) gen_dummy_locs = data.frame(name=dummy_gen_names, x=2, y=seq(0, 1, length.out=length(dummy_gen_names))) gen_locs = data.frame(name=gen_names, x=2.5, y=seq(1/11, 10/11, length.out=length(gen_names))) locs = rbind(dummy_locs, well_locs, fp_locs, gen_dummy_locs, gen_locs) locs$id = ids[locs$name] locs = locs %&gt;% arrange(id) g = graph_from_adjacency_matrix(v) %&gt;% set_vertex_attr(&#39;label&#39;, value=all_names) %&gt;% set_vertex_attr(&#39;x&#39;, value=as.vector(locs$x)) %&gt;% set_vertex_attr(&#39;y&#39;, value=as.vector(locs$y)) %&gt;% set_vertex_attr(&#39;label.degree&#39;, value=pi) %&gt;% as.undirected() V(g)$size = ifelse(V(g)$label %in% well_names, 4, 8) V(g)$color = ifelse(V(g)$label %in% dry_wells, &quot;red&quot;, ifelse(V(g)$label %in% no_data_wells, &quot;grey&quot;, &quot;orange&quot;)) E(g)$color = &quot;black&quot; E(g)[which(tail_of(g, E(g))$label==&quot;DUMMY&quot;)]$color = &quot;grey&quot; # png(&quot;../_media/full_network.png&quot;) # par(mar=c(0,3,0,0), family=&quot;Times&quot;) # plot(g, vertex.label.dist=3, # mark.groups = list(wells=ids[well_names], fps=ids[fp_names], gens=ids[gen_names]), # mark.col = &quot;#DDDDDD&quot;, # mark.border = NA) # text(c(-1, -0.3, 0.4, 0.9), 1.15, c(&quot;Wells&quot;, &quot;Flash plants&quot;, &quot;Dummy gens&quot;, &quot;Generators&quot;), cex=1.25) # dev.off() plot(g, vertex.label.dist=3, mark.groups = list(wells=ids[well_names], fps=ids[fp_names], gens=ids[gen_names]), mark.col = &quot;#DDDDDD&quot;, mark.border = NA) The dummy node is necessary because when indexing a subset of flows that go into a node, this subset cannot be empty. The dummy node has zero mass flowing out of it. 4.2.5 Format Data JAGS requires data to be real numbers, vectors or matrices in a named list. It can also impute NA values from a distribution. Data wrangling is a significant part of the work - potentially more than the actual model coding and the results analysis combined. This code also centers some of the covariates so it does not have to be done in JAGS. \\[\\begin{equation} x_\\text{whp} \\leftarrow x_\\text{whp} - \\overline{x_\\text{whp}} \\end{equation}\\] regression_list = regression_df %&gt;% select(well_id, whp, mf, date_numeric) %&gt;% as.list() dry_list = dry_df %&gt;% filter(date &lt; prediction_date) %&gt;% rename(well_id_dry=well_id, mf_dry=mf, date_numeric_dry=date_numeric) %&gt;% # use these in a different regression select(well_id_dry, mf_dry, date_numeric_dry) %&gt;% as.list() operating_conditions_list = operating_conditions %&gt;% arrange(well_id) %&gt;% select(whp_pred) %&gt;% as.list() fp_constants_list = as.list(fp_constants) gen_constants_list = as.list(gen_constants %&gt;% select(gen_id, factor)) facilities = data.frame(id=ids) %&gt;% left_join(operating_conditions %&gt;% rename(id=well_id) %&gt;% filter(id %in% ids) %&gt;% select(-well), by=&#39;id&#39;) %&gt;% left_join(gen_constants %&gt;% select(factor, id=gen_id), by=&#39;id&#39;) %&gt;% left_join(fp_constants %&gt;% rename(id=fp_id), by=&#39;id&#39;) %&gt;% filter(id %in% ids) %&gt;% # in case extras specified in data mutate(mf_pred=NA) %&gt;% mutate(n_inflows=colSums(v)) well_ids = ids[well_names] liq_well_ids = ids[liq_wells] dry_well_ids = ids[dry_wells] fp_ids = ids[fp_names] ip_gen_ids = ids[ip_gen_names] lp_gen_ids = ids[lp_gen_names] w_gen_ids = ids[w_gen_names] gen_ids = ids[gen_names] # force all mass to IP steam dry_fps = c(&quot;poi dry&quot;, &quot;direct ip&quot;) dry_fp_ids = ids[dry_fps] facilities$hf_ip[facilities$id %in% dry_fp_ids] = 10 facilities$hfg_ip[facilities$id %in% dry_fp_ids] = 10 facilities_list = facilities %&gt;% select(-id) %&gt;% as.list() # experimental TS data matrix for dry wells ar_order = 1 empty = setNames(data.frame(matrix(ncol = length(all_names), nrow = 0)), all_names) drymatrix = dry_df %&gt;% select(well, date_numeric, mf) %&gt;% spread(well, mf) %&gt;% select(-date_numeric) drymatrix = empty %&gt;% full_join(drymatrix) %&gt;% as.matrix() ar_well_ids = which(complete.cases(t(drymatrix[1:(ar_order+1),]))) ar_wells = names(ids)[ar_well_ids] # which wells can we not use AR for dry_no_ar_wells = dry_wells[!dry_well_ids %in% ar_well_ids] dry_no_ar_well_ids = ids[dry_no_ar_wells] # insert production curve predictions stopifnot(all(tsplotwells %in% dry_df$well)) tsplotwells = ar_wells days_since_last = as.integer(today_datetime - as.POSIXct(max(dry_df$date))) prod = expand.grid(whp_prod=seq(6, 16, length.out=10), well_id_prod=ids[production_curve_wells]) ts = expand.grid(date_numeric_ts=seq(min(dry_df$date_numeric), max(dry_df$date_numeric)+days_since_last, length.out=10), well_id_ts=ids[tsplotwells]) prod_list = prod %&gt;% as.list ts_list = ts %&gt;% as.list # extend matrix for prediction drymatrix = rbind(drymatrix, matrix(NA, nrow=days_since_last, ncol=ncol(drymatrix))) # combine into one list data = c(regression_list, dry_list, facilities_list, prod_list, ts_list, list(well_ids=well_ids, liq_well_ids=liq_well_ids, dry_well_ids=dry_well_ids, dry_no_ar_well_ids=dry_no_ar_well_ids, fp_ids=fp_ids, gen_ids=gen_ids, ip_gen_ids=ip_gen_ids, lp_gen_ids=lp_gen_ids, w_gen_ids=w_gen_ids, today_numeric=today_numeric, m=m, dummy=1, ts=drymatrix, ts_ar=drymatrix, ts_ema=drymatrix, ar_well_ids=ar_well_ids)) # data$whp_pred[is.na(data$whp_pred)] &lt;- mean(data$whp_pred, na.rm=T) # center covariates mean_whp &lt;- mean(data$whp, na.rm=T) mean_date_numeric &lt;- mean(data$date_numeric, na.rm=T) data$whp_c &lt;- data$whp - mean_whp data$whp_pred_c &lt;- data$whp_pred - mean_whp data$whp_prod_c &lt;- data$whp_prod - mean_whp data$date_numeric_c &lt;- data$date_numeric - mean_date_numeric data$today_numeric_c &lt;- data$today_numeric - mean_date_numeric data$date_numeric_dry_c &lt;- data$date_numeric_dry - mean_date_numeric data$date_numeric_ts &lt;- data$date_numeric_ts - mean_date_numeric pidataplot = ggplot(regression_df %&gt;% filter(source==&quot;PI Database&quot;), aes(x=whp, y=mf, color=well)) + geom_point() + labs(title=paste(&quot;PI Regression Data from&quot;, min(extra_liq$date), &quot;to&quot;, max(extra_liq$date)), x=&quot;Well-head pressure (bar)&quot;, y=&quot;Mixed-phase mass flow (T/h)&quot;, color=&quot;Well&quot;) + guides(color=guide_legend(ncol=2)) + ggsave(&#39;../_media/pi_data.png&#39;, width=24.7, height=12, units=&#39;cm&#39;) ggplotly(pidataplot) 4.3 Model JAGS accepts a model in a text string. It uses an R-like syntax, but is a declarative language not sequential. We do basic manipulation of the output traces. code = &quot; data { D &lt;- dim(ts) } model { ############################################## # fit individual regressions to liquid wells # ############################################## for (i in 1:length(mf)) { mu[i] &lt;- Intercept[well_id[i]] + beta_whp[well_id[i]] * whp_c[i] + beta_date[well_id[i]] * date_numeric_c[i] mf[i] ~ dnorm(mu[i], tau[well_id[i]]) mf_fit[i] ~ dnorm(mu[i], tau[well_id[i]]) # mf_fit[i] ~ dnorm(mu[i]*measurement_error_factor[i], tau[well_id[i]]) # measurement_error_factor[i] ~ dunif(0.9, 1.1) } # fit regression to dry wells for (i in 1:length(mf_dry)) { mu_dry[i] &lt;- Intercept[well_id_dry[i]] + beta_date[well_id_dry[i]] * date_numeric_dry_c[i] mf_dry[i] ~ dnorm(mu_dry[i], tau[well_id_dry[i]]) mf_dry_fit[i] ~ dnorm(mu_dry[i], tau[well_id_dry[i]]) # measurement_error_factor_dry[i] ~ dunif(0.9, 1.1) } for (j in dry_well_ids) { Intercept[j] ~ dnorm(0, 1e-12) beta_date[j] ~ dnorm(0, 1e-12) tau[j] ~ dgamma(1e-12, 1e-12) } # experimental AR1 model for dry wells for (j in ar_well_ids) { for (t in 2:D[1]) { mu_ar[t,j] &lt;- c_ar[j] + theta_ar[j]*ts_ar[t-1,j] ts_ar[t,j] ~ dnorm(mu_ar[t,j], tau_ar[j]) T(0,) } theta_ar[j] ~ dnorm(0, 1e-12) c_ar[j] ~ dnorm(0, 1e-12) tau_ar[j] ~ dgamma(1e-12, 1e-12) } # experimental EWMA model (use at your own risk) for (j in ar_well_ids) { for (t in 2:D[1]) { mu_ema[t,j] &lt;- alpha*mu_ema[t-1,j] + (1-alpha)*ts_ema[t,j] ts_ema[t,j] ~ dnorm(mu_ema[t-1,j], tau_ema[j]) T(0,) } mu_ema[1,j] &lt;- ts_ema[1,j] theta_ema[j] ~ dnorm(0, 1e-12) c_ema[j] ~ dnorm(0, 1e-12) tau_ema[j] ~ dgamma(1e-12, 1e-12) } alpha ~ dbeta(0.5, 0.5) # HIERARCHICAL # fills in for any missing wells for (j in liq_well_ids) { Intercept[j] ~ dnorm(mu_Intercept, tau_Intercept) beta_whp[j] ~ dnorm(mu_beta_whp, tau_beta_whp) # beta_whp2[j] ~ dnorm(mu_beta_whp2, tau_beta_whp2) beta_date[j] ~ dnorm(mu_beta_date, tau_beta_date) tau[j] ~ dgamma(1e-12, 1e-12) sd[j] &lt;- 1/max(sqrt(tau[j]), 1e-12) } # fill in any missing data for (i in 1:length(mf)) { date_numeric_c[i] ~ dnorm(mu_date_numeric, tau_date_numeric) } mu_date_numeric ~ dnorm(0, 1e-12) tau_date_numeric ~ dnorm(1e-12, 1e-12) # set hyperparameters mu_Intercept ~ dnorm(0, 1e-12) mu_beta_whp ~ dnorm(0, 1e-12) # mu_beta_whp2 ~ dnorm(0, 1e-12) mu_beta_date ~ dnorm(0, 1e-12) tau_Intercept ~ dgamma(1e-12, 1e-12) tau_beta_whp ~ dgamma(1e-12, 1e-12) # tau_beta_whp2 ~ dgamma(1e-12, 1e-12) tau_beta_date ~ dgamma(1e-12, 1e-12) ##################################### # production curve for verification # ##################################### for (i in 1:length(whp_prod)) { mu_prod[i] &lt;- Intercept[well_id_prod[i]] + beta_whp[well_id_prod[i]] * whp_prod_c[i] + beta_date[well_id_prod[i]] * today_numeric_c # mf_prod[i] ~ dnorm(mu_prod[i], tau[well_id_prod[i]]) mf_prod[i] &lt;- mu_prod[i] } for (i in 1:length(date_numeric_ts)) { mu_ts[i] &lt;- Intercept[well_id_ts[i]] + beta_date[well_id_ts[i]] * date_numeric_ts[i] mf_ts[i] ~ dnorm(mu_ts[i], tau[well_id_ts[i]]) } ######################################################### # simple model to fill in missing FP enthalpy constants # ######################################################### for (i in fp_ids) { # missing fp constants hf_ip[i] ~ dgamma(param[1], param[7]) hg_ip[i] ~ dgamma(param[2], param[8]) hfg_ip[i] ~ dgamma(param[3], param[9]) hf_lp[i] ~ dgamma(param[4], param[10]) hg_lp[i] ~ dgamma(param[5], param[11]) hfg_lp[i] ~ dgamma(param[6], param[12]) } for (i in c(1, well_ids)) { h[i] ~ dgamma(param[13], param[14]) whp_pred_c[i] ~ dnorm(param[15], param[16]) } # missing well constants for (i in 1:16) { param[i] ~ dgamma(1e-12, 1e-12) } # uniform priors ######################################## # make predictions (the stuff we want) # ######################################## mf_pred[dummy] &lt;- 0 # dummy well ip_sf[dummy] &lt;- 0 lp_sf[dummy] &lt;- 0 wf[dummy] &lt;- 0 # use production curve for (j in liq_well_ids) { mf_pred[j] &lt;- max(Intercept[j] + beta_whp[j] * whp_pred_c[j] + beta_date[j] * today_numeric_c, 0) } # use naive TS reg for (j in dry_well_ids) { #dry_no_ar_well_ids) { mf_pred[j] &lt;- max(Intercept[j] + beta_date[j] * today_numeric_c, 0) } # use AR(1) # for (j in ar_well_ids) { # mf_pred[j] &lt;- mu_ar[D[1], j] # } for (i in fp_ids) { mf_pred[i] &lt;- sum(mf_pred[m[i,1:n_inflows[i]]]) h[i] &lt;- sum(mf_pred[m[i, 1:n_inflows[i]]] * h[m[i, 1:n_inflows[i]]]) / ifelse(mf_pred[i]!=0, mf_pred[i], 1) ip_sf[i] &lt;- min(max((h[i] - hf_ip[i]), 0) / hfg_ip[i], 1) * mf_pred[i] lp_sf[i] &lt;- min(max((min(hf_ip[i], h[i]) - hf_lp[i]), 0) / hfg_lp[i], 1) * (mf_pred[i] - ip_sf[i]) total_sf[i] &lt;- ip_sf[i] + lp_sf[i] wf[i] &lt;- mf_pred[i] - total_sf[i] } # dummy gens and actual gens for (i in ip_gen_ids) { mf_pred[i] &lt;- sum(ip_sf[m[i, 1:n_inflows[i]]]) } for (i in lp_gen_ids) { mf_pred[i] &lt;- sum(lp_sf[m[i, 1:n_inflows[i]]]) } for (i in w_gen_ids) { mf_pred[i] &lt;- sum(wf[m[i, 1:n_inflows[i]]]) } for (i in gen_ids) { mf_pred[i] &lt;- sum(mf_pred[m[i,1:n_inflows[i]]]) power[i] &lt;- mf_pred[i] / mu_factor[i] mu_factor[i] ~ dunif(0.95*factor[i], 1.05*factor[i]) # uncertainty from email } total_power &lt;- sum(power[gen_ids]) } &quot; # cat(code, file=&quot;model.txt&quot;) vars = c(&#39;mf_fit&#39;, &#39;mf_dry_fit&#39;, &#39;mf_ts&#39;, &#39;mf_prod&#39;, &#39;mf_pred&#39;, &#39;beta_date&#39;, &#39;sd&#39;, &#39;power&#39;, &#39;total_sf&#39;, &#39;mu_ar&#39;, &#39;ts_ar&#39;, &#39;mu_ema&#39;, &#39;ts_ema&#39;, &#39;alpha&#39;, &#39;ip_sf&#39;, &#39;lp_sf&#39;, &#39;wf&#39;, paste0(&#39;h[&#39;, fp_ids, &#39;]&#39;), paste0(&#39;mu_&#39;, c(&#39;Intercept&#39;, &#39;beta_whp&#39;, &#39;beta_date&#39;)), &#39;total_power&#39;) n_chains = 2 burn_in = 100 model = jags.model(textConnection(code), data, n.chains=n_chains) ## Compiling data graph ## Resolving undeclared variables ## Allocating nodes ## Initializing ## Reading data back into data table ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 6259 ## Unobserved stochastic nodes: 3916 ## Total graph size: 29435 ## ## Initializing model update(model, burn_in) out = coda.samples(model, n.iter=round(n_steps/n_chains), variable.names=vars) outmatrix = as.matrix(out) outframe = as.data.frame(outmatrix) %&gt;% gather(key=facility, value=value) %&gt;% mutate(variable=gsub(&quot;\\\\[.*$&quot;, &quot;&quot;, facility), facility=parse_number(facility, na=c(&quot;NA&quot;))) outframe$facility = factor(names(ids)[outframe$facility]) 4.4 Convergence Tests One of the difficulties with MCMC approximations is they often require a burn-in (warm-up) period before settling into the stationary distribution of the Markov chain. Only the stationary distribution corresponds to the joint distribution we wish to sample from. In most practical uses, there is no way to predict convergence, so we diagnose convergence by monitoring the sample trace and running diagnostic tests. 4.4.1 Trace plots Poor convergence or mixing is indicated by a strong trend at the beginning of the trace plot. trace1 &lt;- outframe %&gt;% filter(variable==&#39;mf_pred&#39;, facility==censor(&#39;wk256&#39;, &quot;well&quot;)) %&gt;% mutate(index = 1:nrow(.)) trace2 &lt;- outframe %&gt;% filter(variable==&#39;total_power&#39;) %&gt;% mutate(index = 1:nrow(.)) trace3 &lt;- outframe %&gt;% filter(variable==&#39;mu_Intercept&#39;) %&gt;% mutate(index = 1:nrow(.)) traceplot = ggplot(trace1, aes(x=index, y=value, color=variable)) + geom_line(alpha=0.75) + geom_line(alpha=0.75) + geom_line(alpha=0.75) + coord_cartesian(xlim = c(max(trace1$index)-1000, max(trace1$index))) + labs(title=&quot;Trace Plot (Single chain)&quot;, x=&quot;Iteration&quot;, y=&quot;Parameter value&quot;, color=&quot;Variable&quot;)# + # ggsave(&#39;../_media/trace_plot.png&#39;, width=24.7, height=8, units=&#39;cm&#39;) ggplotly(traceplot) 4.4.2 Geweke Geweke’s convergence diagnostic for MCMC samples tests for equality of the means in the first 10% and last 50% of the trace. The means will be equal if the sample is drawn from a stationary distribution, indicating the burn-in period has been successfully excluded. If true univariate convergence has been achieved, we expect 95% of variables to pass Geweke’s test with a z-score less than 1.96 with 95% confidence. random_var_ix = sample.int(ncol(outmatrix), 100) # 100 random var because it takes too long geweke.out = geweke.diag(out[,random_var_ix]) geweke.df = data.frame(Index = 1:length(unlist(geweke.out)), z = unlist(geweke.out[1])) %&gt;% mutate(out = ifelse(abs(z)&gt;1.96, T, F)) %&gt;% drop_na() proportion_out = sum(geweke.df$out) / nrow(geweke.df) gewekeplot = ggplot(geweke.df, aes(x=Index, y=z)) + geom_point() + geom_hline(data=data.frame(value=c(1.96,-1.96)), aes(yintercept=value), color=&#39;red&#39;) + labs(title=paste0(&quot;Geweke z-score. &quot;, round(proportion_out, 2)*100, &quot;% of points lie outside the 95% confidence interval.&quot;))# + # ggsave(&#39;../_media/geweke.png&#39;, width=24.7, height=6, units=&#39;cm&#39;) ggplotly(gewekeplot) 4.4.3 Gelman The Gelman-Rubin convergence diagnostic gives the potential scale reduction factor (PSRF) for each parameter. This requires at least two independent chains and tests whether the chains have converged to identical distributions. If the chains have not converged, the scale reduction factors will have upper confidence limits greater than one. It is possible that when run indefinitely, the variance of the parameter estimate could shrink by the PSRF. gelman.out = gelman.diag(out[,c(paste0(&#39;mf_pred[&#39;, 8:9, &#39;]&#39;), &#39;beta_date[9]&#39;, &#39;mu_beta_whp&#39;, &#39;mu_beta_date&#39;, &#39;mu_Intercept&#39;, &#39;total_power&#39;)])[[1]] %&gt;% as.data.frame() kable(gelman.out) %&gt;% kable_styling() Point est. Upper C.I. mf_pred[8] 1.0021015 1.014338 mf_pred[9] 0.9995892 1.001750 beta_date[9] 1.0649835 1.263674 mu_beta_whp 1.0463801 1.195384 mu_beta_date 1.1455117 1.518625 mu_Intercept 1.0180951 1.051931 total_power 1.0796053 1.170108 Some of the upper CIs are slightly greater than one, but not significantly. Large PSRFs are acceptable if they are in components of the network that do not affect parameters of interest. 4.4.4 Raftery Raftery’s diagnostic gives the number of samples required to estimate a quantile (or credible interval) to a certain accuracy. In this notebook we only run 1000 samples so it says we do not have enough. raftery.out = raftery.diag(out[,c(paste0(&#39;mf_pred[&#39;, 8:9, &#39;]&#39;), &#39;beta_date[9]&#39;, &#39;mu_beta_whp&#39;, &#39;mu_beta_date&#39;, &#39;mu_Intercept&#39;, &#39;total_power&#39;)]) raftery.out[[1]] ## ## Quantile (q) = 0.025 ## Accuracy (r) = +/- 0.005 ## Probability (s) = 0.95 ## ## You need a sample size of at least 3746 with these values of q, r and s 4.5 Posteriors We generate density plots in their most basic forms without post-processing. 4.5.1 Well Mass Flow g1 = ggplot(outframe %&gt;% filter(facility %in% well_names, variable==&quot;mf_pred&quot;, value&gt;0) %&gt;% mutate(source = ifelse(facility %in% dry_wells, &quot;PI time series&quot;, &quot;Production curve&quot;)), aes(x=value, fill=facility)) + geom_density(aes(y=..scaled..), alpha=0.5, color=NA) + xlim(0, NA) + facet_grid(source~.) + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + labs(title=paste(&quot;Posterior Well Mass Flows for&quot;, prediction_date), x=&quot;Mass flow (T/h)&quot;, y=&quot;Scaled density&quot;, fill=&quot;Facility&quot;)# + # ggsave(&#39;../_media/mf_wells.png&#39;, width=24.7, height=8, units=&#39;cm&#39;) ggplotly(g1, tooltip=c(&#39;facility&#39;, &#39;value&#39;)) 4.5.2 Decline Rate An operator might like to see which wells are declining the fastest. g2 = ggplot(outframe %&gt;% filter(variable==&quot;beta_date&quot;, facility %in% special_wells), aes(x=value, fill=facility)) + geom_density(alpha=0.5, color=NA) +labs(title=&quot;Posterior Decline Rate of Test Data&quot;, x=&quot;beta_date (T/h/Bar)&quot;, y=&quot;Density&quot;, fill=&quot;Facility&quot;) + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank())# + # ggsave(&#39;../_media/beta_date.png&#39;, width=24.7, height=6, units=&#39;cm&#39;) ggplotly(g2, tooltip=c(&#39;facility&#39;, &#39;value&#39;)) 4.5.3 FP Mass Flow g4 = ggplot(outframe %&gt;% filter(facility %in% gen_names, variable==&quot;mf_pred&quot;, value&gt;0), aes(x=value, fill=facility)) + geom_density(aes(y=..scaled..), alpha=0.5, color=NA) + xlim(0, NA) + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + labs(title=paste(&quot;Posterior Generator Values for&quot;, prediction_date), x=&quot;Mass flow (T/h)&quot;, y=&quot;Scaled density&quot;, fill=&quot;Facility&quot;)# + # ggsave(&#39;../_media/mf_gens.png&#39;, width=24.7, height=10, units=&#39;cm&#39;) ggplotly(g4, tooltip=c(&#39;facility&#39;, &#39;value&#39;)) 4.5.4 Gen Mass Flow g5.actual = data.frame(facility = c(&quot;WRK&quot;, &quot;THI&quot;, &quot;POI&quot;, &quot;BIN&quot;), value = c(121.73567, 172.18096, 51.53028, 9.98687)) g5 = ggplot(outframe %&gt;% filter(facility %in% gen_names, variable==&quot;power&quot;, value&gt;0), aes(x=value, fill=facility)) + geom_density(aes(y=..scaled..), alpha=0.5, color=NA) + xlim(0, NA) + geom_vline(data=g5.actual, aes(xintercept=value, color=facility)) + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + labs(x=&quot;Power (MW)&quot;, y=&quot;Scaled density&quot;, fill=&quot;Facility&quot;)# + # ggsave(&#39;../_media/power_gens.png&#39;, width=24.7, height=10, units=&#39;cm&#39;) ggplotly(g5, tooltip=c(&#39;facility&#39;, &#39;value&#39;)) # tsgrob4.5 = grid_arrange_shared_legend(g4, g5, nrow=2, ncol=1, position = &quot;right&quot;) # ggsave(&#39;../_media/gens.png&#39;, tsgrob4.5, width=24.7, height=6, units=&#39;cm&#39;) 4.5.5 Gen Power tb6 &lt;- outframe %&gt;% filter(variable==&quot;sd&quot;) %&gt;% select(facility, value) %&gt;% mutate(well=factor(facility)) %&gt;% group_by(well) %&gt;% summarise(Mean = mean(value), `Lower 2.5%` = quantile(value, 0.025), `Upper 97.5%` = quantile(value, 0.975)) %&gt;% mutate_if(is.numeric, round, 3) %&gt;% inner_join(regression_df %&gt;% mutate(well=factor(names(ids)[well_id])) %&gt;% group_by(well) %&gt;% summarise(n=n()), by=&quot;well&quot;) g6 = ggplot(outframe %&gt;% filter(variable==&quot;sd&quot;) %&gt;% filter(facility %in% special_wells), aes(x=value, fill=facility)) + geom_density(alpha=0.5, color=NA) + coord_cartesian(xlim=c(0, max(tb6$`Upper 97.5%`))) + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + labs(title=&quot;Posterior Flow Deviation Estimates&quot;, x=&quot;Standard deviation&quot;, y=&quot;Density&quot;, fill=&quot;Facility&quot;)# + # ggsave(&#39;../_media/standard_deviation.png&#39;, width=24.7, height=10, units=&#39;cm&#39;) ggplotly(g6, tooltip=c(&#39;facility&#39;, &#39;value&#39;)) 4.6 Advanced Analysis 4.6.1 High Variance wells nrow.source = function(df, facilityname, sourcename) { stopifnot(length(sourcename)==1) return(nrow(df %&gt;% filter(well==facilityname, source==sourcename))) } well_summaries = outframe %&gt;% filter(facility %in% well_names, variable==&quot;mf_pred&quot;) %&gt;% group_by(facility) %&gt;% summarise(mean = mean(value), sd = sd(value), n_test = nrow.source(regression_df, unique(facility),&quot;Well Tests&quot;), n_pi = nrow.source(regression_df, unique(facility), &quot;PI Database&quot;), use.test = ifelse(n_test&gt;0, &quot;Test data&quot;, &quot;No test data&quot;), use.pi = ifelse(n_pi&gt;0, &quot;PI data&quot;, &quot;No PI data&quot;)) %&gt;% arrange(desc(sd)) well_summaries$production.curve = ifelse(well_summaries$facility %in% liq_wells, &quot;Production curve&quot;, &quot;Time series&quot;) fp_summaries = list(fp14 = well_summaries %&gt;% filter(facility %in% flows_to(censor(&#39;fp14&#39;, &#39;fp&#39;))), fp15 = well_summaries %&gt;% filter(facility %in% flows_to(censor(&#39;fp15&#39;, &#39;fp&#39;))), fp16 = well_summaries %&gt;% filter(facility %in% flows_to(censor(&#39;fp16&#39;, &#39;fp&#39;)))) for (fp in names(fp_summaries)) { print(xtable(fp_summaries[[fp]] %&gt;% select(-c(use.test, use.pi, production.curve)), type = &quot;latex&quot;, caption=paste(&quot;Data methods feeding flash plant&quot;, censor(fp, &#39;fp&#39;)), label=paste0(&quot;tab:well_summaries_&quot;, fp)), table.placement = &quot;H&quot;, file = paste0(&quot;../_media/summaries_&quot;, fp, &quot;.tex&quot;)) } fp_summaries %&gt;% kable() %&gt;% kable_styling() %&gt;% scroll_box(height = &quot;400px&quot;) facility mean sd n_test n_pi use.test use.pi production.curve wk242 322.13652 47.2385211 28 0 Test data No PI data Production curve wk264 394.20917 17.2018634 34 30 Test data PI data Production curve wk245 420.04092 15.0713041 41 30 Test data PI data Production curve wk265 334.15445 14.4018421 34 30 Test data PI data Production curve wk243 375.43871 8.3364384 52 30 Test data PI data Production curve wk222 106.31548 8.2999560 44 0 Test data No PI data Production curve wk263 221.33877 3.2113852 34 30 Test data PI data Production curve wk237 14.63283 1.0950901 0 30 No test data PI data Production curve wk233 13.56930 0.6175667 0 30 No test data PI data Production curve wk228 14.98824 0.1108668 0 30 No test data PI data Production curve facility mean sd n_test n_pi use.test use.pi production.curve wk256 218.34271 18.4374652 32 30 Test data PI data Production curve wk268 87.26872 17.7147487 27 30 Test data PI data Production curve wk255 378.43331 13.9252038 41 30 Test data PI data Production curve wk267 307.33283 13.2077826 31 30 Test data PI data Production curve wk269 132.74935 11.8402173 25 30 Test data PI data Production curve wk250 25.05857 7.3247934 0 30 No test data PI data Production curve wk247 233.26984 6.4723543 48 30 Test data PI data Production curve wk272 207.48286 2.8329014 7 30 Test data PI data Production curve wk241 45.77379 0.4400225 0 30 No test data PI data Production curve wk238 60.20425 0.3770650 0 30 No test data PI data Production curve wk251 22.60754 0.1940089 0 30 No test data PI data Production curve wk234 30.77341 0.1904144 0 30 No test data PI data Production curve wk240 23.41470 0.1603002 0 30 No test data PI data Production curve wk252 58.88827 0.0628887 0 30 No test data PI data Production curve facility mean sd n_test n_pi use.test use.pi production.curve wk266 322.97855 16.788105 29 30 Test data PI data Production curve wk262 632.61082 9.718498 36 30 Test data PI data Production curve wk271 85.91758 6.376994 6 30 Test data PI data Production curve wk260 296.04027 4.768856 33 30 Test data PI data Production curve wk253 269.73092 4.695968 32 30 Test data PI data Production curve wk261 250.40980 4.569097 37 30 Test data PI data Production curve wk270 460.58794 4.185943 5 30 Test data PI data Production curve wk258 198.66793 3.437390 38 30 Test data PI data Production curve wk259 308.93622 3.220008 37 30 Test data PI data Production curve wk254 226.69230 2.425235 39 30 Test data PI data Production curve n_summaries = well_summaries %&gt;% group_by(use.pi, use.test) %&gt;% count() sourceplot = ggplot(well_summaries, aes(x=1, y=log(sd))) + geom_boxplot(fill=&#39;steelblue&#39;) + geom_label(data=n_summaries, aes(x=-Inf, y=-Inf, hjust=0, vjust=0, label=paste0(&quot;n=&quot;, n), family=&quot;Times New Roman&quot;), label.size=0, fill=&#39;white&#39;) + facet_grid(.~ use.pi + use.test) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + labs(title=&quot;Differences in Production Error by Data Source&quot;, x=&quot;Production curve data source&quot;, y=&quot;log(standard deviation)&quot;)# + # ggsave(&#39;../_media/error_source.png&#39;, width=24.7*0.5, height=6, units=&#39;cm&#39;) ggplotly(sourceplot) sourcetab = well_summaries %&gt;% select(facility, mean, sd, n_test, n_pi) # print(xtable(sourcetab %&gt;% head(), type = &quot;latex&quot;, # caption=&quot;Upon inspection of the wells with the most variance, there is no immediate cause for high variance. This requires further investigation.&quot;, # label=&quot;tab:well_summaries&quot;), # table.placement = &quot;h&quot;, # file = &quot;../_media/well_summaries.tex&quot;) kable(cbind(sourcetab)) %&gt;% kable_styling() %&gt;% scroll_box(height = &quot;400px&quot;) facility mean sd n_test n_pi wk86 83.8460372 68.2447659 4 0 wk242 322.1365192 47.2385211 28 0 wk28 87.3743099 45.7907525 26 0 wk27 164.9209111 36.8377389 36 0 wk59 115.4227252 33.3333745 31 0 wk65 26.9289968 32.7253182 2 0 wk26b 111.7886843 27.2720525 13 0 wk116 74.0301302 26.4451621 10 0 wk81 61.0126780 25.1791016 24 0 wk123 185.1815737 23.3284710 31 0 wk72 152.2381236 21.6992865 26 0 wk76 124.6642094 21.5574617 31 0 wk67 93.3161893 21.2510659 26 0 wk96 28.4474001 21.2088843 10 0 wk71 128.2472765 19.0542452 32 0 wk256 218.3427094 18.4374652 32 30 wk268 87.2687212 17.7147487 27 30 wk264 394.2091707 17.2018634 34 30 wk266 322.9785462 16.7881046 29 30 wk26a 118.8068633 15.1818908 16 0 wk245 420.0409217 15.0713041 41 30 wk265 334.1544486 14.4018421 34 30 wk83 134.4234464 14.0279330 26 0 wk255 378.4333068 13.9252038 41 30 wk267 307.3328349 13.2077826 31 30 wk74 157.2891438 12.5518300 26 0 wk269 132.7493509 11.8402173 25 30 wk55 61.9725023 10.1604612 47 0 wk235 206.6448877 9.8779687 20 0 wk262 632.6108220 9.7184979 36 30 wk124 253.1097726 9.3357234 31 0 wk244 196.2798991 9.2025407 37 30 wk70 150.8437133 9.0633400 45 0 wk243 375.4387090 8.3364384 52 30 wk229 203.8152946 8.3261616 92 0 wk222 106.3154780 8.2999560 44 0 wk250 25.0585723 7.3247934 0 30 wk207 149.6589830 6.6177060 18 0 wk247 233.2698354 6.4723543 48 30 wk271 85.9175778 6.3769942 6 30 wk239 133.1476594 5.8199382 18 0 wk46 44.0406697 5.0652279 18 0 wk260 296.0402686 4.7688561 33 30 wk253 269.7309182 4.6959675 32 30 wk261 250.4098049 4.5690972 37 30 wk270 460.5879440 4.1859435 5 30 wk258 198.6679295 3.4373901 38 30 wk259 308.9362209 3.2200080 37 30 wk263 221.3387688 3.2113852 34 30 wk272 207.4828563 2.8329014 7 30 wk254 226.6923042 2.4252352 39 30 wk605 36.0438758 1.2774026 0 0 wk237 14.6328286 1.0950901 0 30 wk606 22.0390041 0.8709535 0 0 wk233 13.5693018 0.6175667 0 30 wk241 45.7737888 0.4400225 0 30 wk238 60.2042520 0.3770650 0 30 wk249 1.0435145 0.3707624 0 0 wk607 1.5229216 0.2727646 0 0 wk610 6.7454739 0.2520371 0 0 wk25 6.7978269 0.2043194 0 0 wk251 22.6075442 0.1940089 0 30 wk234 30.7734124 0.1904144 0 30 wk240 23.4146964 0.1603002 0 30 wk232 0.0940381 0.1111293 0 30 wk228 14.9882370 0.1108668 0 30 wk236 26.3413685 0.0845945 0 0 wk216 8.0725978 0.0829701 0 0 wk252 58.8882661 0.0628887 0 30 wk118 9.5151455 0.0332561 0 0 wk604 0.7825579 0.0311954 0 0 wk101 0.0030795 0.0021309 0 0 wk92 0.0012277 0.0009114 0 0 wk66 0.0000000 0.0000000 0 0 wk88 0.0000000 0.0000000 0 0 4.6.2 Regression Fits prod = as.data.frame(outmatrix) %&gt;% select(contains(&#39;prod&#39;)) %&gt;% gather(key=facility, value=value) %&gt;% mutate(which=parse_number(facility)) %&gt;% mutate(whp=data$whp_prod[which], well = names(ids)[data$well_id_prod[which]]) %&gt;% rename(mf=value) %&gt;% group_by(well, whp) %&gt;% summarise(lower=quantile(mf, 0.025), upper=quantile(mf, 0.975), mean=mean(mf)) plotdata = regression_df %&gt;% filter(well_id %in% ids[production_curve_wells]) %&gt;% mutate(datetime = factor(as.Date(date))) %&gt;% mutate(source = factor(source, levels=c(&quot;Well Tests&quot;, &quot;PI Database&quot;))) # regression plot regplot = ggplot(prod, aes(x=whp)) + geom_line(aes(y=mean, color=well)) + geom_ribbon(aes(ymin=lower, ymax=upper, fill=well), alpha=0.25) + geom_point(data=plotdata, aes(y=mf, color=well, size=date, shape=source), alpha=0.5) + labs(title=&quot;Linear Regression on Test and PI Data&quot;, x=&quot;Well-head pressure (bar)&quot;, y=&quot;Mass flow (T/h)&quot;, color=&quot;Well&quot;, shape=&quot;Data source&quot;, size=&quot;Date&quot;, fill=&quot;Well&quot;) + coord_cartesian(xlim=c(min(plotdata$whp)*0.9,max(plotdata$whp)*1.1), ylim=c(0,max(plotdata$mf)*1.1))# + # ggsave(&#39;../_media/production_curve.png&#39;, width=24.7*0.48, height=24.7*0.48, units=&#39;cm&#39;) ggplotly(regplot) 4.6.3 Time Series Plots tsplotwells = ar_wells ts_fit = as.data.frame(outmatrix) %&gt;% select(contains(&#39;mf_ts&#39;)) %&gt;% gather() %&gt;% mutate(index = parse_number(key)) %&gt;% select(-key) %&gt;% group_by(index) %&gt;% summarise(lower=quantile(value, 0.025), upper=quantile(value, 0.975), mean=mean(value)) %&gt;% cbind(ts) %&gt;% mutate(well = factor(names(ids[well_id_ts])), date_numeric = date_numeric_ts) # actual observations tsplotdata = dry_df %&gt;% filter(well_id %in% ids[tsplotwells]) %&gt;% mutate(datetime = factor(as.Date(date)), facility = well) # experimental AR1 time series ar_fit = as.data.frame(outmatrix) %&gt;% select(contains(&quot;mu_ar&quot;)) %&gt;% gather() %&gt;% mutate(date_numeric = as.numeric(str_extract(key, &quot;(?&lt;=\\\\[)(.*?)(?=,)&quot;)) + min(dry_df$date_numeric) - 1, facility = names(ids)[as.numeric(str_extract(key, &quot;(?&lt;=,)(.*?)(?=\\\\])&quot;))]) %&gt;% select(facility, date_numeric, value) %&gt;% group_by(facility, date_numeric) %&gt;% summarise(mean=mean(value), lower=quantile(value, 0.025), upper=quantile(value, 0.975)) %&gt;% filter(facility %in% tsplotwells) # experimental EMA time series ewma_fit = as.data.frame(outmatrix) %&gt;% select(contains(&quot;mu_ema&quot;)) %&gt;% gather() %&gt;% mutate(date_numeric = as.numeric(str_extract(key, &quot;(?&lt;=\\\\[)(.*?)(?=,)&quot;)) + min(dry_df$date_numeric) - 1, facility = names(ids)[as.numeric(str_extract(key, &quot;(?&lt;=,)(.*?)(?=\\\\])&quot;))]) %&gt;% select(facility, date_numeric, value) %&gt;% group_by(facility, date_numeric) %&gt;% summarise(mean=mean(value), lower=quantile(value, 0.025), upper=quantile(value, 0.975)) %&gt;% filter(facility %in% tsplotwells) # find plot limits tsmax = max(c(ts_fit$upper, ar_fit$upper, ewma_fit$upper)) lintsplot = ggplot(ts_fit, aes(x=date_numeric, color=well, fill=well)) + geom_line(aes(y=mean), linetype=&quot;dashed&quot;) + geom_ribbon(aes(ymin=lower, ymax=upper), color=NA, alpha=0.25) + geom_line(data=tsplotdata, aes(y=mf)) + geom_vline(aes(xintercept = max(tsplotdata$date_numeric)), linetype=&quot;dashed&quot;, color=&quot;red&quot;) + coord_cartesian(ylim=c(0, 60)) + labs(title=paste(&quot;Linear Time Series Regression for Selected Wells in PI&quot;), x=&quot;Days since baseline (2000)&quot;, linetype=&quot;&quot;)# + # ggsave(&#39;../_media/dry_time_series.png&#39;, width=24.7, height=8, units=&#39;cm&#39;) arplot = ggplot(ar_fit %&gt;% filter(facility %in% tsplotwells), aes(x=date_numeric, y=mean, fill=facility, color=facility)) + geom_line(data=tsplotdata, aes(y=mf)) + geom_ribbon(aes(ymin=lower, ymax=upper), color=NA, alpha=0.5) + geom_line(linetype=&quot;dashed&quot;) + coord_cartesian(ylim=c(0, 60)) + geom_vline(aes(xintercept = max(tsplotdata$date_numeric)), linetype=&quot;dashed&quot;, color=&quot;red&quot;) + labs(title=&quot;AR(1) Experiment&quot;, x=&quot;Days since first date&quot;, y=&quot;Mass flow (T/h)&quot;) + ggsave(&#39;../_media/ar_experiment.png&#39;, width=24.7, height=8, units=&#39;cm&#39;) ewmaplot = ggplot(ewma_fit, aes(x=date_numeric, y=mean, fill=facility, color=facility)) + geom_line(data=tsplotdata, aes(y=mf)) + geom_ribbon(aes(ymin=lower, ymax=upper), color=NA, alpha=0.5) + geom_line(linetype=&quot;dashed&quot;) + coord_cartesian(ylim=c(0, 60)) + geom_vline(aes(xintercept = max(tsplotdata$date_numeric)), linetype=&quot;dashed&quot;, color=&quot;red&quot;) + labs(title=&quot;EWMA Experiment&quot;, x=&quot;Days since first date&quot;)# + # ggsave(&#39;../_media/ewma_experiment.png&#39;, width=24.7, height=8, units=&#39;cm&#39;) ggplotly(lintsplot) ggplotly(arplot) ggplotly(ewmaplot) tsgrob = grid_arrange_shared_legend(lintsplot, arplot, ewmaplot, nrow=3, ncol=1, position = &quot;bottom&quot;) tsgrob ## TableGrob (2 x 1) &quot;arrange&quot;: 2 grobs ## z cells name grob ## 1 1 (1-1,1-1) arrange gtable[arrange] ## 2 2 (2-2,1-1) arrange gtable[guide-box] # ggsave(&#39;../_media/ts_experiment.png&#39;, tsgrob, width=24.7, height=24, units=&#39;cm&#39;) 4.6.4 Goodness of fit (OLS regression) liq_fit = as.data.frame(outmatrix) %&gt;% select(contains(&#39;mf_fit&#39;)) %&gt;% gather(key=&#39;index&#39;, value=&#39;fitted&#39;) %&gt;% mutate(index=as.integer(parse_number(index))) %&gt;% group_by(index) %&gt;% summarise(lower=quantile(fitted, 0.025), upper=quantile(fitted, 0.975), Fitted=mean(fitted), std=sd(fitted)) %&gt;% cbind(regression_df) %&gt;% mutate(`Standardised residual` = (Fitted-mf)/std, Well = factor(names(ids[well_id])), Observed = mf) %&gt;% gather(key=&quot;key&quot;, value=&quot;value&quot;, `Standardised residual`, Observed) %&gt;% select(Well, key, Fitted, value, source) diagplot = ggplot(liq_fit, aes(x=Fitted, y=value)) + geom_point(aes(color=Well, shape=Well)) + scale_shape_manual(values = rep_len(1:25, length(unique(liq_fit$Well)))) + geom_smooth(color=&#39;black&#39;) + facet_wrap(~key, scales=&quot;free&quot;) + geom_hline(data=data.frame(key=&quot;Standardised residual&quot;, value=c(1.96,-1.96)), aes(yintercept=value), color=&#39;red&#39;) + geom_abline(data=data.frame(key=&quot;Observed&quot;, a = 1, b = 0), aes(slope = a, intercept=b), color=&#39;red&#39;) + # coord_cartesian(ylim=c(-4, 4)) + labs(title=&quot;Diagnostic Plots&quot;, x=&quot;Fitted mass flow (T/h)&quot;, y=&quot;&quot;) + theme(legend.position = &quot;bottom&quot;) + guides(color=guide_legend(nrow=3, byrow=T), shape=guide_legend(nrow=3, byrow=T))# + # ggsave(&#39;../_media/diagnostics.png&#39;, width=24.7, height=12, units=&#39;cm&#39;) ggplotly(diagplot) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; selectwells = liq_fit %&gt;% group_by(Well, key) %&gt;% summarise(fittedsd = sd(Fitted)) %&gt;% arrange(desc(fittedsd)) %&gt;% head(56*2) %&gt;% pull(Well) observedplot = ggplot(liq_fit %&gt;% filter(key==&quot;Observed&quot;, Well %in% selectwells), aes(x=Fitted, y=value)) + geom_point(aes(color=source), alpha=0.5) + geom_smooth(color=NA, alpha=0.5) + facet_wrap(~Well, scales=&quot;free&quot;) + geom_abline(data=data.frame(key=&quot;Observed&quot;, a = 1, b = 0), aes(slope = a, intercept=b)) + labs(title=&quot;Linear Regression Fit Plots Per Well&quot;, x=&quot;Fitted mass flow (T/h)&quot;, y=&quot;Observed mass flow (T/h)&quot;, color=&quot;Data source&quot;) + theme(legend.position = &quot;bottom&quot;)# + # guides(color=guide_legend(nrow=3, byrow=T), shape=guide_legend(nrow=3, byrow=T)) + # ggsave(&#39;../_media/observed.png&#39;, width=24.7, height=24.7, units=&#39;cm&#39;) stdresplot = ggplot(liq_fit %&gt;% filter(key==&quot;Standardised residual&quot;, Well %in% selectwells), aes(x=Fitted, y=value)) + geom_point(aes(color=source), alpha=0.5) + geom_smooth(color=NA, alpha=0.5) + facet_wrap(~Well, scales=&quot;free_x&quot;) + geom_hline(data=data.frame(key=&quot;Standardised residual&quot;, value=c(1.96,-1.96)), aes(yintercept=value), color=&#39;red&#39;) + # geom_abline(data=data.frame(key=&quot;Observed&quot;, a = 1, b = 0), aes(slope = a, intercept=b), color=&#39;red&#39;) + labs(title=&quot;Linear Regression Residual Plots Per Well&quot;, x=&quot;Fitted mass flow (T/h)&quot;, y=&quot;Standardised residual&quot;, color=&quot;Data source&quot;) + coord_cartesian(ylim=c(-5, 5)) + theme(legend.position=&quot;bottom&quot;)# + # guides(color=guide_legend(nrow=3, byrow=T), shape=guide_legend(nrow=3, byrow=T)) + # ggsave(&#39;../_media/stdres.png&#39;, width=24.7, height=24.7, units=&#39;cm&#39;) ggplotly(observedplot) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplotly(stdresplot) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; # stdres_min = liq_fit %&gt;% filter(key==&quot;Standardised residual&quot;) %&gt;% pull(value) %&gt;% min() # stdres_max = liq_fit %&gt;% filter(key==&quot;Standardised residual&quot;) %&gt;% pull(value) %&gt;% max() # ggplot(liq_fit %&gt;% filter(key==&quot;Standardised residual&quot;), aes(x=value)) + # geom_density(fill=&quot;red&quot;, alpha=0.5, color=NA) + # geom_line(data=data.frame(x=seq(stdres_min, stdres_max, length.out=100)), aes(x=x, y=dnorm(x))) 4.6.5 Limits and Constraint Violations sf.df &lt;- outframe %&gt;% filter(str_detect(variable, &quot;total_sf&quot;) &amp; value &gt; 0) %&gt;% droplevels() limits = fp_constants %&gt;% mutate(facility = names(ids)[fp_id]) %&gt;% select(facility, limit) %&gt;% drop_na() p.limits = sf.df %&gt;% left_join(limits, by=c(&quot;facility&quot;)) %&gt;% mutate(greater = value &gt; limit) %&gt;% group_by(facility) %&gt;% summarise(p.greater = mean(greater)) %&gt;% drop_na() limitplot = ggplot(sf.df %&gt;% filter(facility %ni% incomplete.fps), aes(x=value, fill=facility)) + facet_wrap(~facility, scales = &quot;free_y&quot;, ncol=2) + geom_density(alpha=0.5, color=NA) + geom_vline(data=limits, aes(xintercept=limit), color=&quot;red&quot;) + geom_label(data=p.limits %&gt;% filter(facility %ni% incomplete.fps), aes(x=-Inf, y=Inf, hjust=0, vjust=1, label=paste0(&quot;p(&gt;lim)=&quot;, p.greater), family=&quot;Times New Roman&quot;), color=&quot;black&quot;, label.size=0, fill=&#39;white&#39;) + theme(legend.position=&quot;none&quot;, axis.text.y=element_blank(), axis.ticks.y=element_blank()) + labs(title=&quot;Posterior Flash Plant Mass Flows&quot;, x=&quot;Steam flow (T/h)&quot;, y=&quot;Density&quot;, fill=&quot;Flash plant&quot;, color=&quot;Steam flow limit&quot;)# + # ggsave(&#39;../_media/constraints.png&#39;, width=24.7, height=10, units=&#39;cm&#39;) ggplotly(limitplot) 4.6.6 Flow Comparison flow.df &lt;- outframe %&gt;% filter(facility %in% fp_names) %&gt;% filter(str_detect(variable, &quot;mf_pred|ip_sf|lp_sf|wf&quot;) &amp; value &gt; 0) %&gt;% mutate(variable=ifelse(variable==&quot;mf_pred&quot;, &quot;mf&quot;, variable), variable=factor(variable, levels=c(&quot;mf&quot;, &quot;ip_sf&quot;, &quot;lp_sf&quot;, &quot;wf&quot;))) comparison = fp_constants %&gt;% select(&quot;fp&quot;, contains(&quot;verification&quot;)) %&gt;% rename(facility=fp) %&gt;% gather(key=&quot;variable&quot;, value=&quot;value&quot;, -facility) %&gt;% mutate(variable = gsub(&quot;^verification_&quot;, &quot;&quot;, variable), variable=factor(variable, levels=c(&quot;mf&quot;, &quot;ip_sf&quot;, &quot;lp_sf&quot;, &quot;wf&quot;))) %&gt;% drop_na() ps = flow.df %&gt;% left_join(comparison, by=c(&quot;facility&quot;, &quot;variable&quot;)) %&gt;% mutate(greater = value.x &gt; value.y) %&gt;% group_by(facility, variable) %&gt;% summarise(p.greater = mean(greater)) %&gt;% mutate(variable=factor(variable, levels=c(&quot;mf&quot;, &quot;ip_sf&quot;, &quot;lp_sf&quot;, &quot;wf&quot;))) %&gt;% drop_na() verificationplot = ggplot(flow.df %&gt;% filter(facility %ni% incomplete.fps), aes(x=value)) + geom_density(aes(y=..scaled.., fill=variable, color=variable), alpha=0.5, show.legend=F) + geom_vline(data=comparison %&gt;% filter(facility %ni% incomplete.fps), aes(xintercept=value)) + geom_label(data=ps %&gt;% filter(facility %ni% incomplete.fps), aes(x=-Inf, y=Inf, hjust=0, vjust=1, label=paste0(&quot;p(&gt;x)=&quot;, p.greater), family=&quot;Times New Roman&quot;), label.size=0) + facet_grid(facility~variable, scales=&quot;free&quot;, space=&quot;free_y&quot;) + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + labs(x=&quot;Value&quot;, y=&quot;Scaled density&quot;, title=&quot;Comparison Between Predicted FP Flows and Sample Data&quot;)# + # ggsave(&#39;../_media/verification.png&#39;, width=24.7, height=20, units=&#39;cm&#39;) ggplotly(verificationplot) "],
["testing.html", "5 Testing", " 5 Testing x = 1 y = 2 print(y) ## 2 print(r.x) ## 1.0 print(y) ## 2 py$y ## [1] 2 "]
]
